{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import os ; os.environ['KERAS_BACKEND']='theano'\n",
    "%run init.ipy\n",
    "#import logging    \n",
    "#logging.getLogger('keras').setLevel(logging.INFO)\n",
    "\n",
    "from utils import *\n",
    "import miregularizer2\n",
    "#mi_grad_trainable = False\n",
    "#sfx = 'v8train'\n",
    "opts = dict(\n",
    "    sfx='v8train',\n",
    "    mi_grad_trainable = True,\n",
    "    nbepoch = 50,\n",
    "    batch_size = 128,\n",
    "    patiencelevel = 50,\n",
    "    do_pretrain=True,\n",
    "    N = None,\n",
    "    EntropyEstimateN = 2000,\n",
    "    HIDDEN_DIM = 20,\n",
    "    max_alpha=10.,\n",
    ")\n",
    "\n",
    "#sfx = 'v9train'\n",
    "opts = dict(\n",
    "    sfx='v9train',\n",
    "    mi_grad_trainable = True,\n",
    "    nbepoch = 50,\n",
    "    batch_size = 128,\n",
    "    patiencelevel = 50,\n",
    "    do_pretrain=False,\n",
    "    N = None,\n",
    "    EntropyEstimateN = 2000,\n",
    "    HIDDEN_DIM = 20,\n",
    "    max_alpha=2.,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#N=10000\n",
    "d=load_mnist(max_train_items=opts['N'], max_test_items=opts['N']) # , keep_classes = [0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Dense(2*HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "model.add(Dense(opts['HIDDEN_DIM'], input_dim=d.train.X.shape[1], activation='tanh'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(MILayerTrainable(HIDDEN_DIM, alpha=.1, initlogvar=-1., add_noise=True, entropy_only=False))\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "kdelayer, noiselayer, micomputer = None, None, None\n",
    "kdetraincb, noisetraincb, reportcb = None, None, None\n",
    "\n",
    "if True:\n",
    "    kdelayer   = miregularizer2.KDEParamLayer(init_logvar=-5)\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(kdelayer)\n",
    "    #alltraindata = K.variable(d.train.X)\n",
    "    regularize_mi_input = d.train.X\n",
    "    if opts['EntropyEstimateN'] is not None:\n",
    "        regularize_mi_input = randsample(regularize_mi_input, opts['EntropyEstimateN'])\n",
    "\n",
    "    #print alltraindata.dtype\n",
    "    def get_noise_input_func(pd):\n",
    "        d = tf.constant(pd)\n",
    "        for layerndx, layer in enumerate(model.layers):\n",
    "            d = layer(d)\n",
    "        return d\n",
    "        \n",
    "    noiselayer = miregularizer2.GaussianNoise2(init_logvar=-10, kdelayer=kdelayer, \n",
    "                                               regularize_mi_input=regularize_mi_input, \n",
    "                                               init_alpha=0., \n",
    "                                               get_noise_input_func=get_noise_input_func,\n",
    "                                               trainable=opts['mi_grad_trainable'])\n",
    "    model.add(noiselayer)\n",
    "\n",
    "    kdetraincb   = miregularizer2.KDETrain(entropy_train_data=regularize_mi_input, kdelayer=kdelayer)\n",
    "    if not opts['mi_grad_trainable']:\n",
    "        noisetraincb = miregularizer2.NoiseTrain(traindata=d.train, noiselayer=noiselayer)\n",
    "    else:\n",
    "        noisetraincb = None\n",
    "    reportcb     = miregularizer2.ReportVars(noiselayer=noiselayer, kdelayer=kdelayer)\n",
    "\n",
    "    \n",
    "#model.add(Dense(HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "model.add(Dense(d.train.nb_classes, activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "def compilefunc():\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    if noiselayer is not None and hasattr(noiselayer, 'logvar'):\n",
    "        K.set_value(noiselayer.logvar, -10)\n",
    "    if kdelayer is not None and hasattr(kdelayer, 'logvar'):\n",
    "        K.set_value(kdelayer.logvar, -10)\n",
    "    \n",
    "compilefunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=opts['patiencelevel'])\n",
    "#model.fit(X, y, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "if opts['do_pretrain']:\n",
    "    if noiselayer is not None:\n",
    "        K.set_value(noiselayer.mi_regularizer.alpha, 0.0)\n",
    "\n",
    "    inithist = model.fit(d.train.X, d.train.Y, nb_epoch=opts['nbepoch'], validation_split=0.2,\n",
    "                     batch_size=opts['batch_size'], verbose=2, # validation_split=0.1, \n",
    "                     callbacks=[cb for cb in [kdetraincb,reportcb,early_stopping] if cb is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/initmodel-%s.h5\"%opts['sfx']\n",
    "print \"Saving %s\"% fname\n",
    "model.save_weights(fname)\n",
    "fname = \"models/opts-%s.txt\"%opts['sfx']\n",
    "print \"Saving %s\"% fname\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(str(opts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#alphavals = np.linspace(0, 5, 20, endpoint=True)\n",
    "#alphavals = np.exp(np.linspace(0, np.log(6), 20, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(0, np.log(3.), 30, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(np.log(5), np.log(10.), 10, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(0, np.log(10.), 10, endpoint=True))-1\n",
    "alphavals = np.exp(np.linspace(0, np.log(opts['max_alpha']), 10, endpoint=False))-1\n",
    "\n",
    "#del saved_hist\n",
    "#saved_hist = {}\n",
    "if 'saved_hist' not in locals():\n",
    "    print \"Initializing saved_hist\"\n",
    "    saved_hist = {}\n",
    "#saved_hist = {}\n",
    "for alpha in alphavals:\n",
    "    print \"**************** Doing alpha=%.8f, %s ****************\" % (alpha, opts['sfx'])\n",
    "    if alpha in saved_hist:\n",
    "        continue\n",
    "    #if alpha < 2 or alpha > 2.2:\n",
    "    #    continue\n",
    "    # Reset\n",
    "    #for clayer in model.layers:\n",
    "    #    clayer.build(clayer.input_shape)\n",
    "    \n",
    "    #if alpha == 0:\n",
    "    compilefunc()\n",
    "    model.load_weights(\"models/initmodel-%s.h5\"%opts['sfx'])\n",
    "        \n",
    "    if noiselayer is not None :\n",
    "        K.set_value(noiselayer.mi_regularizer.alpha, alpha)\n",
    "\n",
    "    hist = model.fit(d.train.X, d.train.Y, nb_epoch=opts['nbepoch'],\n",
    "                     batch_size=opts['batch_size'], verbose=2, validation_split=0.2, \n",
    "                     callbacks=[cb for cb in [kdetraincb, noisetraincb, reportcb, early_stopping] if cb is not None],  # , \n",
    "                    )\n",
    "    saved_hist[alpha] = {'history':hist.history, \n",
    "                         'endlogs': miregularizer2.get_logs(model, d, kdelayer, noiselayer, max_entropy_calc_N=opts['EntropyEstimateN'])}\n",
    "    fname = \"models/fitmodel-%s-%0.5f.h5\"%(opts['sfx'],alpha)\n",
    "    print \"saving to %s\"%fname\n",
    "    model.save_weights(fname)\n",
    "    \n",
    "    if True:\n",
    "        savedhistfname='models/savedhist-%s.dat'%opts['sfx']\n",
    "        with open(savedhistfname, 'wb') as f:\n",
    "            cPickle.dump(saved_hist, f)\n",
    "        print 'updated', savedhistfname\n",
    "\n",
    "    print \n",
    "    print\n",
    "raise Exception('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compilefunc()\n",
    "#model.load_weights(\"models/fitmodel-%s.h5\"%sfx)\n",
    "K.set_value(noiselayer.mi_regularizer.alpha, 400.)\n",
    "\n",
    "hist = model.fit(d.train.X, d.train.Y, nb_epoch=200,\n",
    "                 batch_size=10., verbose=2, # validation_split=0.1, \n",
    "                 callbacks=[kdetraincb, reportcb,]  # noisetraincb, \n",
    "                )\n",
    "print miregularizer2.get_logs(model, d, kdelayer, noiselayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensor1=noiselayer.get_noise_input_func(tf.constant(d.train.X))\n",
    "mi_obj_trn = miregularizer2.MIComputer(tensor1, kdelayer=kdelayer, noiselayer=noiselayer)\n",
    "#print K.eval(mi_obj_trn.get_mi())\n",
    "#print K.eval(mi_obj_trn.get_hcond())\n",
    "print K.eval(mi_obj_trn.get_h())\n",
    "cvar = K.exp(noiselayer.logvar)+K.exp(kdelayer.logvar)\n",
    "\n",
    "print K.eval(miregularizer2.kde_entropy(tensor1, cvar))\n",
    "\n",
    "print K.eval(cvar), 'cvar'\n",
    "cvar2 = K.exp(noiselayer.logvar)\n",
    "print K.eval(miregularizer2.kde_entropy(noiselayer.get_noise(tensor1), cvar2))\n",
    "print K.eval(cvar2), 'cvar2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (np.cov(K.eval(tensor1)))\n",
    "print (np.cov(K.eval(noiselayer.get_noise(tensor1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cinput = K.placeholder()\n",
    "mi_obj = miregularizer2.MIComputer(cinput, kdelayer=kdelayer, noiselayer=noiselayer)\n",
    "\n",
    "mifunc = K.function([cinput,],  [mi_obj.get_mi(), mi_obj.get_h(), mi_obj.get_hcond()])\n",
    "print mifunc([d.train.X,])\n",
    "cvar = K.exp(noiselayer.logvar)+K.exp(kdelayer.logvar)\n",
    "print K.eval(cvar)\n",
    "#        return kde_entropy(self.input, totalvar)\n",
    "h=K.eval(miregularizer2.kde_entropy(K.variable(activity_trn[fname]), cvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#%%timeit -n 1 -r 1\n",
    "batch_size = 10\n",
    "callbacks = []\n",
    "if noiselayer is not None and kdelayer is not None:\n",
    "    noiselayer.alpha = 0.0\n",
    "    callbacks = [miregularizer2.KDETrain(traindata=d, model=model, kdelayer=kdelayer),\n",
    "                 miregularizer2.NoiseTrain(traindata=d, model=model, noiselayer=noiselayer),\n",
    "                 miregularizer2.ReportVars(traindata=d, noiselayer=noiselayer,kdelayer=kdelayer),\n",
    "                ]\n",
    "\n",
    "hist = model.fit(d.X_train, d.Y_train, nb_epoch=10,\n",
    "                 batch_size=batch_size, validation_split=0.1, verbose=2, \n",
    "                 callbacks=callbacks)\n",
    "#print hist.history\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#fitmodel1.00000.h5\n",
    "activity_trn = {}\n",
    "activity_tst = {}\n",
    "for fname in [\"models/fitmodel-v3-3.64159.h5\",]:#\"fitmodel.h5\", \"fitmodel1.00000.h5\", \"fitmodel2.00000.h5\"]:\n",
    "    compilefunc()\n",
    "    model.load_weights(fname)\n",
    "\n",
    "    #activity = get_activations(model, -2, d.X_train)[0]\n",
    "    #get_activations = K.function([model.layers[0].input, K.learning_phase()], [noiselayer.output,])\n",
    "    #activations = get_activations([X_batch,0])\n",
    "    #activity = get_activations([d.X_train, 1])\n",
    "    #plt.figure()\n",
    "    #plot_activity(activity, colors=d.y_train, size=1) # , doPCA=False)\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [noiselayer.input,])\n",
    "    \n",
    "    activity_trn[fname]=get_activations([d.train.X,])[0]\n",
    "    activity_tst[fname]=get_activations([d.test.X,])[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print  mi1/np.log(2)\n",
    "#print  mi2/np.log(2)\n",
    "cvar= K.exp(noiselayer.logvar)+K.exp(kdelayer.logvar)\n",
    "cvar2=K.exp(noiselayer.logvar)\n",
    "cvar = K.variable(np.exp(-10)+np.exp(-8))\n",
    "print K.eval(noiselayer.logvar)\n",
    "print K.eval(cvar)\n",
    "print K.eval(K.exp(noiselayer.logvar))\n",
    "print K.eval(miregularizer2.kde_entropy(K.variable(activity_trn[fname]), cvar)) # - K.eval(miregularizer2.kde_condentropy(K.variable(activity_trn[fname]), cvar2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (K.eval(miregularizer2.kde_entropy(K.variable(activity_trn[fname]), cvar)))\n",
    "print K.eval(miregularizer2.kde_entropy(cvar*K.variable(np.random.randn(*activity_trn[fname].shape)), cvar))\n",
    "print K.eval(miregularizer2.kde_condentropy(K.variable(np.random.random(activity_trn[fname].shape)), cvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print np.var(5*np.random.randn(10000))\n",
    "#print K.eval(miregularizer2.kde_entropy(cvar*K.variable(np.random.randn(*activity_trn[fname].shape)), cvar))\n",
    "cvar = 0.2\n",
    "print 'closedform', 0.5*np.log(2*cvar*np.pi*np.e)\n",
    "print K.eval(miregularizer2.kde_entropy(np.sqrt(cvar)*K.variable(np.random.randn(10000,1)), cvar))\n",
    "\n",
    "#plt.hist(np.random.randn(10000), bins=100)\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "for ndx, fname in enumerate(activity_trn.keys()): # enumerate([\"fitmodel.h5\", \"fitmodel1.00000.h5\",\"fitmodel2.00000.h5\"]): #\"fitmodel1.50000.h5\", \n",
    "    for trntstndx in range(2):\n",
    "        ax = fig.add_subplot(1,2,1+trntstndx) # , projection='3d')\n",
    "        #plt.subplot(1,4,ndx+1)\n",
    "        if trntstndx == 0:\n",
    "            activitydata, titlestr, labels = activity_trn, 'trn', d.train\n",
    "        else:\n",
    "            activitydata, titlestr, labels = activity_tst, 'tst', d.test\n",
    "        if activitydata[fname].shape[1] == 2:\n",
    "            plt.scatter(activitydata[fname][:,0], activitydata[fname][:,1], c=labels.y, edgecolors='none')\n",
    "        \n",
    "        #plot_activity(activitydata[fname], doPCA=True, colors=labels.y, size=2, dims=2, opts=dict(whiten=True))\n",
    "        plt.title(r'$\\alpha = %d$ - (%s)' % (ndx,  titlestr))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    "#plt.savefig('imgs/hidden.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(d2.X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(activity[:,0],activity[:,1])\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.figure()\n",
    "plt.plot(np.var(activity, axis=0))\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "\n",
    "\"\"\"\n",
    "f1 = K.function([model.layers[0].input, K.learning_phase()], [noiselayer.input])\n",
    "x1=f1([d.X_train, 0])[0]\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "import sklearn.decomposition\n",
    "\n",
    "x2 = sklearn.decomposition.PCA(3, whiten=True).fit_transform(x1)\n",
    "plt.scatter(x2[:,0], x2[:,1], zs=x2[:,2], c=d.y_train, edgecolor='none')\n",
    "print x2.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adict = {}\n",
    "#adict['noreg'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropyonly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mionly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropynnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['minnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['fixednoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mifixednoise'] = get_activations(model, 0, X_train)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in adict.iteritems():\n",
    "    plot_activity(v)\n",
    "    plt.title(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Kdists = K.placeholder(ndim=2)\n",
    "Klogvar = K.placeholder(ndim=0)\n",
    "from miregularizer2 import kde_entropy_from_dists_loo\n",
    "N = 1000\n",
    "dims = 10\n",
    "lossfunc = K.function([Kdists, Klogvar,], [kde_entropy_from_dists_loo(Kdists, N, dims, K.exp(Klogvar))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lossfunc([np.eye(N),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(np.array([1])) # [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fh=K.function([model.layers[0].input,],[noiselayer.mi_obj.get_h()])\n",
    "f = K.function([model.layers[0].input,],[noiselayer.input])\n",
    "#.mi_obj.get_h()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx=f([d.X_train,])\n",
    "print xx\n",
    "print fh([d.X_train,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K.eval(K.exp(noiselayer.logvar)+K.exp(noiselayer.kdelayer.logvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v=K.placeholder(ndim=2)\n",
    "lv=K.placeholder(ndim=0)\n",
    "from miregularizer2 import kde_entropy # , get_dims\n",
    "f2 = K.function([v,lv],[kde_entropy(v,lv)]) # [noiselayer.mi_obj.get_h()])kde_entropy(self.layer.input, totalvar\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print xx[0].shape\n",
    "f2([xx[0],1.])\n",
    "#import theano\n",
    "#theano.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphavals = np.exp(np.linspace(0, np.log(1.1), 30, endpoint=True))-1\n",
    "print alphavals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 15\n",
    "cvar = .5\n",
    "cov1 = np.eye(n)*cvar\n",
    "print np.linalg.det(cov1)\n",
    "print .5**n\n",
    "\n",
    "print 0.5*n*np.log(2*np.pi*np.e*cvar)\n",
    "print 0.5*np.log(((2*np.pi*np.e)**n)*np.linalg.det(cov1))\n",
    "#print 0.5*n*np.log(2*np.pi*np.e*(.5**n))\n",
    "\n",
    "del n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
