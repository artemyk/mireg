{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%run init.ipy\n",
    "from utils import *\n",
    "from miregularizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 1.5885 - acc: 0.6515 - val_loss: 0.5332 - val_acc: 0.8862\n",
      " -0.902511715889\n",
      "1  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 1.1320 - acc: 0.8195 - val_loss: 0.3569 - val_acc: 0.9128\n",
      " -0.817595422268\n",
      "2  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 0.9765 - acc: 0.8575 - val_loss: 0.2872 - val_acc: 0.9233\n",
      " -0.758215904236\n",
      "3  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 222s - loss: 0.8954 - acc: 0.8756 - val_loss: 0.2463 - val_acc: 0.9325\n",
      " -0.745184600353\n",
      "4  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 27s - loss: 0.8396 - acc: 0.8889 - val_loss: 0.2192 - val_acc: 0.9402\n",
      " -0.749277412891\n",
      "5  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.7951 - acc: 0.8968 - val_loss: 0.1988 - val_acc: 0.9453\n",
      " -0.745330512524\n",
      "6  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 9s - loss: 0.7595 - acc: 0.9060 - val_loss: 0.1851 - val_acc: 0.9470\n",
      " -0.744346737862\n",
      "7  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.7305 - acc: 0.9110 - val_loss: 0.1724 - val_acc: 0.9503\n",
      " -0.743145227432\n",
      "8  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.7103 - acc: 0.9156 - val_loss: 0.1646 - val_acc: 0.9522\n",
      " -0.743934512138\n",
      "9  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 0.6892 - acc: 0.9218 - val_loss: 0.1584 - val_acc: 0.9525\n",
      " -0.737605392933\n",
      "10  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 0.6721 - acc: 0.9241 - val_loss: 0.1520 - val_acc: 0.9570\n",
      " -0.734127879143\n",
      "11  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 0.6645 - acc: 0.9259 - val_loss: 0.1491 - val_acc: 0.9572\n",
      " -0.749032795429\n",
      "12  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 10s - loss: 0.6465 - acc: 0.9299 - val_loss: 0.1442 - val_acc: 0.9608\n",
      " -0.741240084171\n",
      "13  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.6348 - acc: 0.9328 - val_loss: 0.1432 - val_acc: 0.9602\n",
      " -0.731500208378\n",
      "14  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.6236 - acc: 0.9349 - val_loss: 0.1413 - val_acc: 0.9597\n",
      " -0.727878272533\n",
      "15  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 12s - loss: 0.6154 - acc: 0.9373 - val_loss: 0.1385 - val_acc: 0.9602\n",
      " -0.725727319717\n",
      "16  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 8s - loss: 0.6045 - acc: 0.9392 - val_loss: 0.1364 - val_acc: 0.9622\n",
      " -0.722313046455\n",
      "17  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 0.5994 - acc: 0.9408 - val_loss: 0.1359 - val_acc: 0.9612\n",
      " -0.717392504215\n",
      "18  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 7s - loss: 0.5896 - acc: 0.9421 - val_loss: 0.1336 - val_acc: 0.9638\n",
      " -0.713078439236\n",
      "19  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 12s - loss: 0.5832 - acc: 0.9436 - val_loss: 0.1311 - val_acc: 0.9640\n",
      " -0.707689583302\n",
      "20  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 10s - loss: 0.5746 - acc: 0.9447 - val_loss: 0.1310 - val_acc: 0.9640\n",
      " -0.71083688736\n",
      "21  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 0.5699 - acc: 0.9457 - val_loss: 0.1295 - val_acc: 0.9647\n",
      " -0.707302689552\n",
      "22  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 0.5609 - acc: 0.9480 - val_loss: 0.1260 - val_acc: 0.9650\n",
      " -0.700433075428\n",
      "23  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 0.5577 - acc: 0.9483 - val_loss: 0.1262 - val_acc: 0.9650\n",
      " -0.70132291317\n",
      "24  Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 6s - loss: 0.5559 - acc: 0.9487 - val_loss: 0.1264 - val_acc: 0.9660\n",
      " -0.711407721043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_DIM = 20\n",
    "model = Sequential()\n",
    "model.add(Dense(2*HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "model.add(Dense(HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=True, initlogvar=-1., add_noise=True, entropy_only=False))\n",
    "model.add(Dense(d.nb_classes, activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "for iterndx in range(25):\n",
    "    print iterndx, \" \",\n",
    "    #model.fit(X_train, Y_train, nb_epoch=1, batch_size=250, validation_split=0.1) # , verbose=1)\n",
    "    model.fit(d.X_train, d.Y_train, nb_epoch=1, batch_size=500, validation_split=0.1) # , verbose=1)\n",
    "    if hasattr(model.layers[-2],'logvar'):\n",
    "        print K.get_value(model.layers[-2].logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b23c322fad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# , activity_regularizer=MIRegularizer(0, gaussian_var)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m model.compile(loss='categorical_crossentropy',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_classes' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from miregularizer import *\n",
    "\n",
    "HIDDEN_DIM = 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2*HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "model.add(Dense(HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "if False:\n",
    "    pass #.97\n",
    "elif True:    \n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=True, initlogvar=-1., add_noise=True, entropy_only=False))\n",
    "elif True:    # not clustered enough, no advatnage\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=False, initlogvar=-1., add_noise=False, entropy_only=False))\n",
    "elif True:    # not clustered enough, no advatnage\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=0.5, add_noise=False, entropy_only=False))\n",
    "elif True:   # .965, but very \"clustered\" due to small initlogvar\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=-2, add_noise=False, entropy_only=False))\n",
    "elif False:\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.05, trainablevar=True, initlogvar=0.5, add_noise=False, entropy_only=False))\n",
    "elif True:  \n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "elif True:  \n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.4, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "elif True:   # reaches .972 on 2 layer net 2*20, 20  .also with alpha=.2\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "elif True:  # reaches .97 on 2 layer net 2*20, 20 .\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.05, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "elif True:\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.01, trainablevar=True, initlogvar=0.5, add_noise=True, entropy_only=False))\n",
    "elif False:\n",
    "    # mi w. noise, fixed var\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "elif False:\n",
    "    # mi w. noise, trainable var\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=True, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "elif True:\n",
    "    # entrpoy w. noise, trainable var\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=True, initlogvar=10., add_noise=True, entropy_only=True))\n",
    "elif False:\n",
    "    # noise, fixed var, no penalty\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=0, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "else:\n",
    "    # noise, fixed var, entropy cost\n",
    "    model.add(MILayer(HIDDEN_DIM, alpha=0.2, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=True))\n",
    "    \n",
    "#model.add(MILayer(HIDDEN_DIM, alpha=0.1, add_noise=True))\n",
    "#model.add(MILayer(HIDDEN_DIM, alpha=0.1, trainablevar=False, initlogvar=-.7, add_noise=True, entropy_only=False))\n",
    "#model.add(MILayer(HIDDEN_DIM, alpha=0.0, add_noise=True, entropy_only=False))\n",
    "\n",
    "\n",
    "#model.add(Dense(HIDDEN_DIM, input_dim=X_train.shape[1], activation='tanh', activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "\n",
    "#model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "#model.add(GaussianNoise(np.sqrt(gaussian_var)))\n",
    "#model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "#model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "model.add(Dense(nb_classes, activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iterndx in range(25):\n",
    "    print iterndx, \" \",\n",
    "    #model.fit(X_train, Y_train, nb_epoch=1, batch_size=250, validation_split=0.1) # , verbose=1)\n",
    "    model.fit(X_train, Y_train, nb_epoch=1, batch_size=500, validation_split=0.1) # , verbose=1)\n",
    "    if hasattr(model.layers[-2],'logvar'):\n",
    "        print K.get_value(model.layers[-2].logvar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity = get_activations(model, -2, X_train)[0]\n",
    "plot_activity(activity) # , doPCA=False)\n",
    "plt.figure()\n",
    "plt.scatter(activity[:,0],activity[:,1])\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.figure()\n",
    "plt.plot(np.var(activity, axis=0))\n",
    "plt.ylim([0, plt.ylim()[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adict = {}\n",
    "#adict['noreg'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropyonly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mionly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropynnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['minnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['fixednoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mifixednoise'] = get_activations(model, 0, X_train)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in adict.iteritems():\n",
    "    plot_activity(v)\n",
    "    plt.title(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ".5**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
