{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import os ; os.environ['KERAS_BACKEND']='theano'\n",
    "%run init.ipy\n",
    "#import logging    \n",
    "#logging.getLogger('keras').setLevel(logging.INFO)\n",
    "\n",
    "from utils import *\n",
    "import miregularizer2\n",
    "\n",
    "# opts = dict(\n",
    "#     sfx='v8train',\n",
    "#     mi_grad_trainable = True,\n",
    "#     nbepoch = 50,\n",
    "#     batch_size = 128,\n",
    "#     patiencelevel = 50,\n",
    "#     do_pretrain=True,\n",
    "#     N = None,\n",
    "#     EntropyEstimateN = 2000,\n",
    "#     HIDDEN_DIM = 20,\n",
    "# )\n",
    "\n",
    "# opts = dict(\n",
    "#     sfx='v10train',\n",
    "#     mi_grad_trainable = True,\n",
    "#     nbepoch = 50,\n",
    "#     batch_size = 128,\n",
    "#     patiencelevel = 50,\n",
    "#     N = None,\n",
    "#     EntropyEstimateN = 2000,\n",
    "#     HIDDEN_DIM = 20,\n",
    "# )\n",
    "import keras.optimizers\n",
    "mnist_mlp_base = dict( # gets 1.28-1.29 training error\n",
    "    do_MI = False,\n",
    "    do_validate_on_test = True,\n",
    "    mi_grad_trainable   = True,\n",
    "    nbepoch             = 30,\n",
    "    batch_size          = 128,\n",
    "    EntropyEstimateN    = 2000,\n",
    "    #HIDDEN_DIMS = [800,800],\n",
    "    #hidden_acts = ['relu','relu'],\n",
    "    HIDDEN_DIMS    = [800,800,256],\n",
    "    hidden_acts    = ['relu','relu','linear'],\n",
    ")\n",
    "\n",
    "# Achieves 1.37-1.4, but note that this with noise in the predictions also\n",
    "# opts = mnist_mlp_base.copy()\n",
    "# opts['sfx'] = 'mnist_v1'\n",
    "# opts['do_MI'] = True  \n",
    "\n",
    "opts = mnist_mlp_base.copy()\n",
    "opts['sfx'] = 'mnist_v1'\n",
    "opts['do_MI'] = True  \n",
    "opts['test_phase_noise'] = False\n",
    "\n",
    "if 'do_pretrain' not in opts:\n",
    "    opts['do_pretrain'] = False\n",
    "\n",
    "\n",
    "def lrscheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.001\n",
    "    elif epoch < 20:\n",
    "        return 0.0005\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "# from eve import *\n",
    "# optimizer=Eve(),    \n",
    "# if isinstance(opts['optimizer'], Eve):\n",
    "#     print \"EveMonitor callback\"\n",
    "#     callbacks.append(EveMonitor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#N=10000\n",
    "d=load_mnist(max_train_items=opts.get('N', None), max_test_items=opts.get('N', None)) # , keep_classes = [0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for hndx, hdim in enumerate(opts['HIDDEN_DIMS']):\n",
    "    if 'hidden_inits' in opts:\n",
    "        cinit = opts['hidden_inits'][hndx]\n",
    "    else:\n",
    "        cinit = 'he_uniform'\n",
    "    model.add(Dense(hdim, \n",
    "                    input_dim=d.train.X.shape[1] if hndx == 0 else None, \n",
    "                    activation=opts['hidden_acts'][hndx],\n",
    "                    init=cinit))\n",
    "    \n",
    "kdelayer, noiselayer, micomputer = None, None, None\n",
    "    \n",
    "callbacks = [keras.callbacks.LearningRateScheduler(lrscheduler),]\n",
    "\n",
    "if opts.get('do_MI', True):\n",
    "    kdelayer   = miregularizer2.KDEParamLayer(init_logvar=-5)\n",
    "    model.add(kdelayer)\n",
    "    regularize_mi_input = d.train.X\n",
    "    if opts['EntropyEstimateN'] is not None:\n",
    "        regularize_mi_input = randsample(regularize_mi_input, opts['EntropyEstimateN'])\n",
    "\n",
    "    def get_noise_input_func(pd):\n",
    "        import tensorflow as tf\n",
    "        d = tf.constant(pd)\n",
    "        for layerndx, layer in enumerate(model.layers):\n",
    "            d = layer(d)\n",
    "        return d\n",
    "        \n",
    "    noiselayer = miregularizer2.GaussianNoise2(init_logvar=-10, kdelayer=kdelayer, \n",
    "                                               regularize_mi_input=regularize_mi_input, \n",
    "                                               init_alpha=0., \n",
    "                                               get_noise_input_func=get_noise_input_func,\n",
    "                                               trainable=opts['mi_grad_trainable'],\n",
    "                                               test_phase_noise=opts.get('test_phase_noise', True))\n",
    "    model.add(noiselayer)\n",
    "\n",
    "    callbacks.append(miregularizer2.KDETrain(entropy_train_data=regularize_mi_input, kdelayer=kdelayer))\n",
    "    if not opts['mi_grad_trainable']:\n",
    "        callbacks.append(miregularizer2.NoiseTrain(traindata=d.train, noiselayer=noiselayer))\n",
    "    callbacks.append(miregularizer2.ReportVars(noiselayer=noiselayer, kdelayer=kdelayer))\n",
    "\n",
    "    \n",
    "#model.add(Dense(HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "model.add(Dense(d.train.nb_classes, init='glorot_uniform', activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "def compilefunc():\n",
    "    optimizer = opts.get('optimizer','adam')\n",
    "    print \"Using optimizer\", optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    if noiselayer is not None and hasattr(noiselayer, 'logvar'):\n",
    "        K.set_value(noiselayer.logvar, -10)\n",
    "    if kdelayer is not None and hasattr(kdelayer, 'logvar'):\n",
    "        K.set_value(kdelayer.logvar, -10)\n",
    "    \n",
    "if opts.get('do_validate_on_test', False):\n",
    "    validation_split = None\n",
    "    validation_data = (d.test.X, d.test.Y)\n",
    "    early_stopping = None\n",
    "else:\n",
    "    validation_split = 0.2\n",
    "    validation_data = None\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    callbacks.append( EarlyStopping(monitor='val_loss', patience=opts['patiencelevel']) )\n",
    "    \n",
    "\n",
    "compilefunc()\n",
    "fit_args = dict(\n",
    "    x=d.train.X,\n",
    "    y=d.train.Y,\n",
    "    verbose=2,\n",
    "    batch_size=opts['batch_size'],\n",
    "    callbacks=callbacks,\n",
    "    nb_epoch=opts['nbepoch'],\n",
    "    validation_split = validation_split,\n",
    "    validation_data = validation_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "if opts.get('do_pretrain', True):\n",
    "    if noiselayer is not None:\n",
    "        K.set_value(noiselayer.mi_regularizer.alpha, 0.0)\n",
    "\n",
    "    inithist = model.fit(**fit_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    fname = \"models/initmodel-%s.h5\"%opts['sfx']\n",
    "    print \"Saving %s\"% fname\n",
    "    model.save_weights(fname)\n",
    "    fname = \"models/opts-%s.txt\"%opts['sfx']\n",
    "    print \"Saving %s\"% fname\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(str(opts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#alphavals = np.linspace(0, 5, 20, endpoint=True)\n",
    "#alphavals = np.exp(np.linspace(0, np.log(6), 20, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(0, np.log(3.), 30, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(np.log(5), np.log(10.), 10, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(0, np.log(10.), 10, endpoint=True))-1\n",
    "#alphavals = np.exp(np.linspace(0, np.log(opts['max_alpha']+1), 10, endpoint=False))-1\n",
    "alphavals = np.linspace(0, opts.get('max_alpha', 1.0), 20, endpoint=False)\n",
    "\n",
    "# no alpha \n",
    "# 15s - loss: 2.7612e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9872\n",
    "\n",
    "alphavals = [1e-3,]\n",
    "\n",
    "#del saved_hist\n",
    "#saved_hist = {}\n",
    "if 'saved_hist' not in locals():\n",
    "    print \"Initializing saved_hist\"\n",
    "    saved_hist = {}\n",
    "#saved_hist = {}\n",
    "for alpha in alphavals:\n",
    "    print \"**************** Doing alpha=%.8f, %s ****************\" % (alpha, opts['sfx'])\n",
    "    if alpha in saved_hist:\n",
    "        continue\n",
    "\n",
    "    compilefunc()\n",
    "    model.load_weights(\"models/initmodel-%s.h5\"%opts['sfx'])\n",
    "        \n",
    "    if noiselayer is not None :\n",
    "        K.set_value(noiselayer.mi_regularizer.alpha, alpha)\n",
    "\n",
    "    hist = model.fit(**fit_args)\n",
    "    saved_hist[alpha] = {'history':hist.history, \n",
    "                         'endlogs': miregularizer2.get_logs(model, d, kdelayer, noiselayer, max_entropy_calc_N=opts['EntropyEstimateN'])}\n",
    "    fname = \"models/fitmodel-%s-%0.5f.h5\"%(opts['sfx'],alpha)\n",
    "    print \"saving to %s\"%fname\n",
    "    model.save_weights(fname)\n",
    "    \n",
    "    if True:\n",
    "        savedhistfname='models/savedhist-%s.dat'%opts['sfx']\n",
    "        with open(savedhistfname, 'wb') as f:\n",
    "            cPickle.dump(saved_hist, f)\n",
    "        print 'updated', savedhistfname\n",
    "\n",
    "    print \n",
    "    print\n",
    "raise Exception('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compilefunc()\n",
    "#model.load_weights(\"models/fitmodel-%s.h5\"%sfx)\n",
    "K.set_value(noiselayer.mi_regularizer.alpha, 400.)\n",
    "\n",
    "hist = model.fit(d.train.X, d.train.Y, nb_epoch=200,\n",
    "                 batch_size=10., verbose=2, # validation_split=0.1, \n",
    "                 callbacks=[kdetraincb, reportcb,]  # noisetraincb, \n",
    "                )\n",
    "print miregularizer2.get_logs(model, d, kdelayer, noiselayer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#fitmodel1.00000.h5\n",
    "activity_trn = {}\n",
    "activity_tst = {}\n",
    "for alphaval in [0,0.5,1.2]:\n",
    "    fname = \"models/fitmodel-v10train-%0.5f.h5\"%alphaval\n",
    "    compilefunc()\n",
    "    model.load_weights(fname)\n",
    "    print fname\n",
    "    \n",
    "    #activity = get_activations(model, -2, d.X_train)[0]\n",
    "    #get_activations = K.function([model.layers[0].input, K.learning_phase()], [noiselayer.output,])\n",
    "    #activations = get_activations([X_batch,0])\n",
    "    #activity = get_activations([d.X_train, 1])\n",
    "    #plt.figure()\n",
    "    #plot_activity(activity, colors=d.y_train, size=1) # , doPCA=False)\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [noiselayer.input,])\n",
    "    \n",
    "    activity_trn[alphaval]=get_activations([d.train.X,])[0]\n",
    "    activity_tst[alphaval]=get_activations([d.test.X,])[0]\n",
    "    print 'COV determinant:', np.linalg.det(np.cov(activity_trn[alphaval].T))\n",
    "    print 'COV diagonals:', np.diag(np.cov(activity_trn[alphaval].T))\n",
    "    print activity_trn[alphaval].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,14))\n",
    "for ndx, alphaval in enumerate(sorted(activity_trn.keys())): # enumerate([\"fitmodel.h5\", \"fitmodel1.00000.h5\",\"fitmodel2.00000.h5\"]): #\"fitmodel1.50000.h5\", \n",
    "    for trntstndx in range(2):\n",
    "        ax = fig.add_subplot(len(activity_trn),2,2*ndx+trntstndx+1) # , projection='3d')\n",
    "        print \"Doing\", ndx, trntstndx\n",
    "        #plt.subplot(1,4,ndx+1)\n",
    "        if trntstndx == 0:\n",
    "            activitydata, titlestr, labels = activity_trn, 'trn', d.train\n",
    "        else:\n",
    "            activitydata, titlestr, labels = activity_tst, 'tst', d.test\n",
    "        #if activitydata[alphaval].shape[1] == 2:\n",
    "        #    plt.scatter(activitydata[alphaval][:,0], activitydata[alphaval][:,1], c=labels.y, edgecolors='none')\n",
    "        \n",
    "        plotcolors = labels.y\n",
    "        plotdata = activitydata[alphaval]\n",
    "        if False:\n",
    "            plotdata, ixs = randsample(plotdata, 2000, return_ixs=True)\n",
    "            plotcolors = plotcolors[ixs]\n",
    "        \n",
    "        plot_activity(plotdata, method='ica', colors=plotcolors, size=2, dims=2)# , opts=dict(whiten=False))\n",
    "        plt.title(r'$\\alpha = %0.3f$ (%s)' % (alphaval,  titlestr))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    "#plt.savefig('imgs/hidden.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
