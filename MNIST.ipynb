{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "%run init.ipy\n",
    "#import logging    \n",
    "#logging.getLogger('keras').setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from miregularizer import kde_entropy, kde_condentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=load_mnist(max_train_items=1000)\n",
    "X_train_Kvar = K.variable(d.X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.regularizers import ActivityRegularizer\n",
    "\n",
    "class MIRegularizer(ActivityRegularizer):\n",
    "    def __init__(self, alpha):\n",
    "        super(MIRegularizer, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def get_h(self):\n",
    "        return kde_entropy(self.layer.input, K.exp(self.layer.logvar)+K.exp(self.layer.kdelayer.logvar))\n",
    "    \n",
    "    def get_hcond(self):\n",
    "        return kde_condentropy(self.layer.input, K.exp(self.layer.kdelayer.logvar))\n",
    "    \n",
    "    def get_mi(self):\n",
    "        return self.get_h() - self.get_hcond()\n",
    "    \n",
    "    def __call__(self, loss):\n",
    "        if not hasattr(self, 'layer'):\n",
    "            raise Exception('Need to call `set_layer` on ActivityRegularizer instance before calling the instance.')\n",
    "            \n",
    "        mi = self.get_mi()\n",
    "        regularized_loss = loss + self.alpha * mi\n",
    "        return K.in_train_phase(regularized_loss, loss)\n",
    "\n",
    "    \n",
    "class GaussianNoise2(Layer):\n",
    "    # with variable noise\n",
    "    def __init__(self, init_logvar, kdelayer, alpha=1.0, *kargs, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init_logvar = init_logvar\n",
    "        self.uses_learning_phase = True\n",
    "        self.kdelayer = kdelayer\n",
    "        self.alpha = alpha\n",
    "        super(GaussianNoise2, self).__init__(*kargs, **kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(GaussianNoise2, self).build(input_shape)\n",
    "        self.logvar = K.variable(float(self.init_logvar))\n",
    "        # self.trainable_weights = [self.logvar,]\n",
    "        self.trainable_weights = []\n",
    "\n",
    "        mireg = MIRegularizer(alpha=self.alpha)\n",
    "        mireg.layer = self\n",
    "        self.regularizers.append(mireg)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        noise_x = x + K.sqrt(K.exp(self.logvar)) * K.random_normal(shape=K.shape(x), mean=0., std=1)\n",
    "        return K.in_train_phase(noise_x, x)\n",
    "\n",
    "class KDEParamLayer(Layer):\n",
    "    # with variable noise\n",
    "    def __init__(self, init_logvar):\n",
    "        self.init_logvar = init_logvar\n",
    "        super(KDEParamLayer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(KDEParamLayer, self).__init__()\n",
    "        self.logvar = K.variable(float(self.init_logvar))\n",
    "        self.trainable_weights = []\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return x\n",
    "        \n",
    "\n",
    "def kde_entropy_from_dists_loo(dists, N, dims, var):\n",
    "    # dists should have large values on diagonal\n",
    "    dists2 = dists / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs = logsumexp(-dists2, axis=1) - np.log(N-1) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return h\n",
    "\n",
    "        \n",
    "        \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "HIDDEN_DIM = 5\n",
    "kdelayer   = KDEParamLayer(init_logvar=-3)\n",
    "noiselayer = GaussianNoise2(init_logvar=-3, kdelayer=kdelayer, alpha=10.0)\n",
    "model = Sequential()\n",
    "#model.add(Dense(2*HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "model.add(Dense(HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(MILayerTrainable(HIDDEN_DIM, alpha=.1, initlogvar=-1., add_noise=True, entropy_only=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(kdelayer)\n",
    "model.add(noiselayer)\n",
    "model.add(Dense(d.nb_classes, activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Input\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from scipy.misc import logsumexp\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "class KDETrain(Callback):\n",
    "    def __init__(self, kdelayer, *kargs, **kwargs):\n",
    "        super(KDETrain, self).__init__(*kargs, **kwargs)\n",
    "        self.kdelayer = kdelayer\n",
    "        #self.kde_train = make_kdevar_update_func(model, kdelayer)\n",
    "        self.nlayerinput = lambda x: K.function([model.layers[0].input, K.learning_phase()], [noiselayer.input])([x,1])[0]\n",
    "        Kdists = K.placeholder()\n",
    "        Klogvar = K.placeholder()\n",
    "        N, dims = d.X_train.shape\n",
    "        self.obj = K.function([Kdists, Klogvar, K.learning_phase()], [kde_entropy_from_dists_loo(dists, N, dims, K.exp(Klogvar))])\n",
    "        self.jac = K.function([Kdists, Klogvar, K.learning_phase()], K.gradients(kde_entropy_from_dists_loo(dists, N, dims, K.exp(Klogvar)), Klogvar))\n",
    "\n",
    "    def get_optimum_sigma(self):\n",
    "        vals = self.nlayerinput(d.X_train)\n",
    "        dists = self.get_dists(vals)\n",
    "        dists += 10e20 * np.eye(dists.shape[0])\n",
    "        stime = time.time()\n",
    "        r= scipy.optimize.minimize(lambda x: self.obj([dists, x, 1])[0], -10, jac=lambda x: self.jac([dists, x, 1])[0])\n",
    "        return r.x[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dists(output):\n",
    "        N, dims = output.shape\n",
    "\n",
    "        # Kernel density estimation of entropy\n",
    "        y1 = output[None,:,:]\n",
    "        y2 = output[:,None,:]\n",
    "\n",
    "        dists = np.sum((y1-y2)**2, axis=2) \n",
    "        return dists\n",
    "\n",
    "    #    def negloglikelihood(logvar):\n",
    "    #        return self.kde_entropy_from_dists_loo(dists, vals.shape[0], vals.shape[1], np.exp(logvar))\n",
    "    #                                                  method='powell')\n",
    "    #@staticmethod\n",
    "    #def kde_entropy_from_dists_loo(dists, N, dims, var):\n",
    "    #    # diagonal entries should be large\n",
    "    #    dists2 = dists / (2*var)\n",
    "    #    normconst = (dims/2.0)*np.log(2*np.pi*var)\n",
    "    #    lprobs = logsumexp(-dists2, axis=1) - np.log(N-1) - normconst\n",
    "    #    h = -np.mean(lprobs)\n",
    "    #    return h\n",
    "\n",
    "    #def on_epoch_end(self, epoch, logs={}):\n",
    "    #    for _ in range(10):\n",
    "    #        print self.kde_train([d.X_train,d.Y_train, np.ones(len(d.X_train)), 1])\n",
    "    #        print 'kdeLV=%.5f' % K.get_value(kdelayer.logvar)\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        best_val = self.get_optimum_sigma()\n",
    "        K.set_value(kdelayer.logvar, best_val)\n",
    "        print 'kdeLV=%.5f' % K.get_value(kdelayer.logvar)\n",
    "        \n",
    "        #for _ in range(10):\n",
    "        #    print self.kde_train([d.X_train,d.Y_train, np.ones(len(d.X_train)), 1])\n",
    "        #    print 'kdeLV=%.5f' % K.get_value(kdelayer.logvar)\n",
    "    \n",
    "class ReportVars(Callback):\n",
    "    def __init__(self, kdelayer, noiselayer, *kargs, **kwargs):\n",
    "        super(ReportVars, self).__init__(*kargs, **kwargs)\n",
    "        self.noiselayer = noiselayer\n",
    "        self.kdelayer = kdelayer\n",
    "        self.mifunc = None\n",
    "        #self.kde_train = make_kdevar_update_func(model, kdelayer)\n",
    "        #self.sample_weights = np.ones(len(d.X_train))\n",
    "\n",
    "    #def on_batch_end(self, batch, log={}):\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.mifunc is None:\n",
    "            self.mifunc = K.function([self.model.layers[0].input, K.learning_phase()], \n",
    "                                     [noiselayer.regularizers[0].get_mi(), \n",
    "                                      noiselayer.regularizers[0].get_h(),\n",
    "                                      noiselayer.regularizers[0].get_hcond()])\n",
    "        lv1, lv2 = K.get_value(kdelayer.logvar), K.get_value(noiselayer.logvar)\n",
    "        logs['kdeLV']   = lv1\n",
    "        logs['noiseLV'] = lv2\n",
    "        print 'kdeLV=%.5f, noiseLV=%.5f, mi=%s' % (lv1, lv2, self.mifunc([d.X_train,1]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "callbacks = []\n",
    "callbacks = [KDETrain(kdelayer=kdelayer),ReportVars(noiselayer=noiselayer,kdelayer=kdelayer),]\n",
    "\n",
    "hist = model.fit(d.X_train, d.Y_train, nb_epoch=50,\n",
    "                 batch_size=batch_size, validation_split=0.1, verbose=2, \n",
    "                 callbacks=callbacks)\n",
    "#print hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = K.function([model.layers[0].input], [noiselayer.input])\n",
    "x1=f1([d.X_train])[0]\n",
    "print x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = K.function([model.layers[0].input], [noiselayer.input])\n",
    "x1=f1([d.X_train])[0]\n",
    "\n",
    "from scipy.misc import logsumexp\n",
    "def kde_entropy_np(output, var):\n",
    "    N, dims = output.shape\n",
    "    \n",
    "    normconst = (dims/2.0)*np.log(2*np.pi*var)\n",
    "            \n",
    "    # Kernel density estimation of entropy\n",
    "    y1 = output[None,:,:]\n",
    "    y2 = output[:,None,:]\n",
    "    \n",
    "    dists = np.sum((y1-y2)**2, axis=2) / (2*var)\n",
    "\n",
    "    # Removes effect of diagonals, i.e. leave-one-out entropy\n",
    "    normCount = N-1\n",
    "    diagvals = 10e20*np.eye(N)\n",
    "    dists = dists + diagvals\n",
    "    #print np.exp(-dists)\n",
    "    lprobs = logsumexp(-dists) - np.log(normCount) - normconst\n",
    "    \n",
    "    h = -np.mean(lprobs)\n",
    "    return h\n",
    "\n",
    "def kdecondentropy2(output, var):\n",
    "    dims = int(output.shape[1])\n",
    "    normconst = (dims/2.0)*np.log(2*np.pi*np.e*var)\n",
    "    return normconst #+ (dims/2.0)\n",
    "\n",
    "import sklearn.decomposition\n",
    "x2 = sklearn.decomposition.PCA(20, whiten=True).fit_transform(x1)\n",
    "\n",
    "l=[]\n",
    "logvars = np.linspace(-10, 0, 20)\n",
    "for logvar in logvars:\n",
    "    pass\n",
    "logvar = 5\n",
    "if True:\n",
    "    print logvar, kde_entropy_np(x2, np.exp(logvar)), kdecondentropy2(x2, np.exp(logvar))\n",
    "    l.append( kde_entropy_np(x2, np.exp(logvar)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(logvars, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iterndx in range(25):\n",
    "    print iterndx, \" \",\n",
    "    #model.fit(X_train, Y_train, nb_epoch=1, batch_size=250, validation_split=0.1) # , verbose=1)\n",
    "    model.fit(d.X_train, d.Y_train, nb_epoch=1, batch_size=500, validation_split=0.1) # , verbose=1)\n",
    "    if hasattr(model.layers[-2],'logvar'):\n",
    "        print K.get_value(model.layers[-3].logvar)\n",
    "        print K.get_value(model.layers[-2].logvar)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from miregularizer import *\n",
    "\n",
    "    HIDDEN_DIM = 20\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(2*HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "    model.add(Dense(HIDDEN_DIM, input_dim=d.X_train.shape[1], activation='tanh'))\n",
    "    if False:\n",
    "        pass #.97\n",
    "    elif True:    \n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=True, initlogvar=-1., add_noise=True, entropy_only=False))\n",
    "    elif True:    # not clustered enough, no advatnage\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=False, initlogvar=-1., add_noise=False, entropy_only=False))\n",
    "    elif True:    # not clustered enough, no advatnage\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:   # .965, but very \"clustered\" due to small initlogvar\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=-2, add_noise=False, entropy_only=False))\n",
    "    elif False:\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.05, trainablevar=True, initlogvar=0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:  \n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:  \n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.4, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:   # reaches .972 on 2 layer net 2*20, 20  .also with alpha=.2\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:  # reaches .97 on 2 layer net 2*20, 20 .\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.05, trainablevar=False, initlogvar=-0.5, add_noise=False, entropy_only=False))\n",
    "    elif True:\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.01, trainablevar=True, initlogvar=0.5, add_noise=True, entropy_only=False))\n",
    "    elif False:\n",
    "        # mi w. noise, fixed var\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "    elif False:\n",
    "        # mi w. noise, trainable var\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.2, trainablevar=True, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "    elif True:\n",
    "        # entrpoy w. noise, trainable var\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=.1, trainablevar=True, initlogvar=10., add_noise=True, entropy_only=True))\n",
    "    elif False:\n",
    "        # noise, fixed var, no penalty\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=0, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=False))\n",
    "    else:\n",
    "        # noise, fixed var, entropy cost\n",
    "        model.add(MILayer(HIDDEN_DIM, alpha=0.2, trainablevar=False, initlogvar=0.1, add_noise=True, entropy_only=True))\n",
    "\n",
    "    #model.add(MILayer(HIDDEN_DIM, alpha=0.1, add_noise=True))\n",
    "    #model.add(MILayer(HIDDEN_DIM, alpha=0.1, trainablevar=False, initlogvar=-.7, add_noise=True, entropy_only=False))\n",
    "    #model.add(MILayer(HIDDEN_DIM, alpha=0.0, add_noise=True, entropy_only=False))\n",
    "\n",
    "\n",
    "    #model.add(Dense(HIDDEN_DIM, input_dim=X_train.shape[1], activation='tanh', activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "\n",
    "    #model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "    #model.add(GaussianNoise(np.sqrt(gaussian_var)))\n",
    "    #model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "    #model.add(Dense(HIDDEN_DIM, input_dim=HIDDEN_DIM, activation='tanh')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "    model.add(Dense(nb_classes, activation='softmax')) # , activity_regularizer=MIRegularizer(0, gaussian_var)))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for iterndx in range(25):\n",
    "        print iterndx, \" \",\n",
    "        #model.fit(X_train, Y_train, nb_epoch=1, batch_size=250, validation_split=0.1) # , verbose=1)\n",
    "        model.fit(X_train, Y_train, nb_epoch=1, batch_size=500, validation_split=0.1) # , verbose=1)\n",
    "        if hasattr(model.layers[-2],'logvar'):\n",
    "            print K.get_value(model.layers[-2].logvar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity = get_activations(model, -2, d.X_train)[0]\n",
    "plot_activity(activity, colors=d.y_train) # , doPCA=False)\n",
    "plt.figure()\n",
    "plt.scatter(activity[:,0],activity[:,1])\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.figure()\n",
    "plt.plot(np.var(activity, axis=0))\n",
    "plt.ylim([0, plt.ylim()[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adict = {}\n",
    "#adict['noreg'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropyonly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mionly'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['entropynnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['minnoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['fixednoise'] = get_activations(model, 0, X_train)[0]\n",
    "#adict['mifixednoise'] = get_activations(model, 0, X_train)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in adict.iteritems():\n",
    "    plot_activity(v)\n",
    "    plt.title(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelobj = model.model\n",
    "trainweights = [kdelayer.logvar, noiselayer.logvar,]\n",
    "#inputlayer = Input(tensor=X_train_Kvar)\n",
    "loss =  kde_entropy(kdelayer.input, K.exp(kdelayer.logvar)+K.exp(noiselayer.logvar))\n",
    "loss -= kde_condentropy(kdelayer.input, K.exp(noiselayer.logvar))\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "print type(K.learning_phase())\n",
    "if (modelobj.uses_learning_phase and type(K.learning_phase()) is not int):\n",
    "    print 'here'\n",
    "    inputs = modelobj.inputs + modelobj.targets + modelobj.sample_weights + [K.learning_phase()]\n",
    "else:\n",
    "    inputs = modelobj.inputs + modelobj.targets + modelobj.sample_weights\n",
    "    \n",
    "print modelobj.total_loss\n",
    "\n",
    "training_updates = modelobj.optimizer.get_updates(trainweights, modelobj.constraints, modelobj.total_loss + loss)\n",
    "updates = modelobj.updates + training_updates\n",
    "\n",
    "# returns loss and metrics. Updates weights at each call.\n",
    "f= K.function(inputs, [loss,], updates=training_updates)\n",
    "f([d.X_train,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print modelobj.loss_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelobj = model.model\n",
    "trainweights = [kdelayer.logvar,]\n",
    "Ktrn = K.variable(d.X_train)\n",
    "logvar = K.variable(0.0)\n",
    "loss = kde_entropy(Ktrn, K.exp(logvar))\n",
    "#mi = kde_entropy(kdelayer.input, K.exp(kdelayer.logvar)+K.exp(noiselayer.logvar)) - \\\n",
    "#     kde_condentropy(kdelayer.input, K.exp(noiselayer.logvar))\n",
    "#loss = modelobj.total_loss + alpha * mi\n",
    "\n",
    "#inputs = modelobj.inputs + modelobj.targets + modelobj.sample_weights + [K.learning_phase()]\n",
    "updates = modelobj.optimizer.get_updates([logvar,], modelobj.constraints, [loss,])\n",
    "\n",
    "f= K.function([logvar,], [loss,], updates=updates)\n",
    "f([float(0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = kde_entropy(Ktrn, K.exp(kdelayer.logvar))\n",
    "gradval = K.gradients(loss, kdelayer.logvar)\n",
    "gradf= K.function([kdelayer.logvar,], gradval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradf([.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "scipy.optimize.minimize(lambda x: f([d.X_train]), -10, jac=lambda x: gradf([d.X_train]), options={'disp':True}) # 'BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "def get_dists(output):\n",
    "    N, dims = output.shape\n",
    "            \n",
    "    # Kernel density estimation of entropy\n",
    "    y1 = output[None,:,:]\n",
    "    y2 = output[:,None,:]\n",
    "    \n",
    "    dists = np.sum((y1-y2)**2, axis=2) \n",
    "    return dists\n",
    "\n",
    "dists = get_dists(x1)\n",
    "\n",
    "def f3(dists, N, dims, var):\n",
    "    dists2 = dists / (2*var)\n",
    "    \n",
    "    normconst = (dims/2.0)*np.log(2*np.pi*var)\n",
    "\n",
    "    if False:\n",
    "        # Removes effect of diagonals, i.e. leave-one-out entropy\n",
    "        normCount = N-1\n",
    "        diagvals = 10e20*np.eye(N)\n",
    "        dists2 = dists2 + diagvals\n",
    "    else:\n",
    "        normCount = N\n",
    "    \n",
    "    lprobs = logsumexp(-dists2) - np.log(normCount) - normconst\n",
    "    \n",
    "    h = -np.mean(lprobs)\n",
    "    return h\n",
    "\n",
    "l=[]\n",
    "logvars = np.linspace(-10, 10, 20)\n",
    "for logvar in logvars:\n",
    "    val =  f3(dists, x1.shape[0], x1.shape[1], np.exp(logvar))\n",
    "    print logvar, val\n",
    "    l.append( val )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=    scipy.optimize.minimize(negloglikelihood, -10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_kdevar_update_func(model, kdelayer):\n",
    "    modelobj = model.model\n",
    "    trainweights = [kdelayer.logvar,]\n",
    "    loss = kde_entropy(kdelayer.input, K.exp(kdelayer.logvar))\n",
    "    #mi = kde_entropy(kdelayer.input, K.exp(kdelayer.logvar)+K.exp(noiselayer.logvar)) - \\\n",
    "    #     kde_condentropy(kdelayer.input, K.exp(noiselayer.logvar))\n",
    "    #loss = modelobj.total_loss + alpha * mi\n",
    "\n",
    "    #inputs = modelobj.inputs + modelobj.targets + modelobj.sample_weights + [K.learning_phase()]\n",
    "    updates = modelobj.optimizer.get_updates(trainweights, modelobj.constraints, [loss,])\n",
    "    \n",
    "    return K.function([model.layers[0].input,K.learning_phase()], [loss,], updates=updates)\n",
    "\n",
    "f=make_kdevar_update_func(model, kdelayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlayerinput = lambda x: K.function([model.layers[0].input, K.learning_phase()], [noiselayer.input])([x,1])[0]\n",
    "vals = nlayerinput(d.X_train)\n",
    "def get_dists(output):\n",
    "    N, dims = output.shape\n",
    "\n",
    "    # Kernel density estimation of entropy\n",
    "    y1 = output[None,:,:]\n",
    "    y2 = output[:,None,:]\n",
    "\n",
    "    dists = np.sum((y1-y2)**2, axis=2) \n",
    "    return dists\n",
    "dists = get_dists(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from miregularizer import kde_entropy_from_dists_loo\n",
    "\n",
    "#dists2 = K.placeholder()\n",
    "logvar2 = K.placeholder()\n",
    "f = K.function([logvar2, K.learning_phase()], [kde_entropy_from_dists_loo2(dists, vals.shape[0], vals.shape[1], K.exp(logvar2))])\n",
    "f2 = K.function([logvar2, K.learning_phase()], K.gradients(kde_entropy_from_dists_loo2(dists, vals.shape[0], vals.shape[1], K.exp(logvar2)), logvar2))\n",
    "def negloglikelihood(logvar):\n",
    "    #return f([d.X_train, 1])[0] #self.\n",
    "    val =f([logvar, 1])[0]\n",
    "    print logvar, val\n",
    "    return  val\n",
    "def jac(logvar):\n",
    "    #return f([d.X_train, 1])[0] #self.\n",
    "    val =f2([logvar, 1])[0]\n",
    "    print 'jac', logvar, val\n",
    "    return  val\n",
    "print f([ 1, 1])[0]\n",
    "print f2([1.1, 1])[0]\n",
    "#adsf\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "r= scipy.optimize.minimize(lambda x: f([x, 1])[0], -10, jac=lambda x: f2([x, 1])[0])\n",
    "print r.x\n",
    "print time.time() - stime\n",
    "#adsf\n",
    "\n",
    "#f([d.X_train, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kde_entropy_from_dists_loo_local(dists, N, dims, var):\n",
    "    dists2 = dists / (2*var)\n",
    "\n",
    "    normconst = (dims/2.0)*np.log(2*np.pi*var)\n",
    "\n",
    "    # Removes effect of diagonals, i.e. leave-one-out entropy\n",
    "    lprobs = logsumexp(-dists2, axis=1) - np.log(N-1) - normconst\n",
    "    h = -np.mean(lprobs)\n",
    "    return h\n",
    "#print \n",
    "#print dists2.shape\n",
    "\n",
    "def f(x):\n",
    "    val = kde_entropy_from_dists_loo(dists2, vals.shape[0], vals.shape[1], np.exp(x))\n",
    "    print x, val\n",
    "    return val\n",
    "stime = time.time()\n",
    "r= scipy.optimize.minimize(lambda x: kde_entropy_from_dists_loo_local(dists2, vals.shape[0], vals.shape[1], np.exp(x)), -10, method='powell')\n",
    "print time.time() - stime\n",
    "print r.x\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
